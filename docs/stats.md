<!-- https://github.com/pages-themes/minimal/blob/master/index.md?plain=1 -->
# How do I do X?
- For an simple R tutorial on power analyses, [click here](#power).

## <a name="power"></a> Power Analyses in R

### Intro: A Deadly Disease
Imagine that you tested negative for a deadly disease. Should you celebrate? It depends on the following: How likely is the test to just say negative---even when you are sick---because it is a weak, bad test? A good, _powerful_ test would almost never do that---if it says that you are sick, it is because you are.* Powerful tests are therefore informative, and weak tests are misleading---in fact, you would probably be **very upset** if you learned that your medical test was weak, because you may have gotten a false-negative on your cancer test. **In psychology, our tests are often much weaker than we think they are, and we should be upset, too;** we often take actions (e.g., abondoning study designs, rejecting hypotheses, etc.) based on weak tests.

### Why Do We Do Weak Tests? Because You Know More Than You Think. 
> “I can’t do a power analysis because I have no idea what the effect size is. If I knew the effect size, I wouldn't have to run the study in the first place!

This is a common objection, and underlies why many still don't do power analyses---but it turns out, we know more than we think we [do](http://jakewestfall.org/blog/index.php/2015/06/16/dont-fight-the-power-analysis/). For instance, 


* * * 

*:(formally, Power = P(identify H0 = F | H1 = T) = 1 - false negative rate)

One of my favorite [articles](./paperpile.html) is by Paul Meehl on this exact problem.





